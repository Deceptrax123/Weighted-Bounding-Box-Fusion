{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part of code taken from https://learnopencv.com/weighted-boxes-fusion/\n",
    "\n",
    "import torch \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import os\n",
    "from ensemble_boxes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/smudge/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-9-28 Python-3.11.4 torch-2.1.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model=torch.hub.load(\"ultralytics/yolov5\",'custom',path='best_yolov5_weights.pt',device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the env variables\n",
    "PATH_TO_TEST='/Volumes/T7 Shield/Smudge/Datasets/Wheat_Head_detection/test/'\n",
    "PATH_TO_INPAINT='/Volumes/T7 Shield/Smudge/Datasets/Wheat_Head_detection/modified_test_set/test_enhanced_inpainting/'\n",
    "PATH_TO_AUTO='/Volumes/T7 Shield/Smudge/Datasets/Wheat_Head_detection/modified_test_set/autoencoder_enhanced/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the image ids\n",
    "test_ids=os.listdir(PATH_TO_TEST)\n",
    "inpaint_ids=os.listdir(PATH_TO_INPAINT)\n",
    "auto_ids=os.listdir(PATH_TO_AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the predictions\n",
    "model.conf=0.3\n",
    "\n",
    "def get_predictions(img_names,path):\n",
    "    pred_scores=dict()\n",
    "    pred_boxes=dict()\n",
    "    pred_classes=dict()\n",
    "\n",
    "    for img in img_names:\n",
    "        if '_' not in img:\n",
    "            img_path=path+img\n",
    "\n",
    "            prediction=model(img_path)\n",
    "\n",
    "            pred_scores[img]=prediction.xyxy[0][:,4].detach().numpy()\n",
    "            pred_boxes[img]=prediction.xyxy[0][:,0:4].detach().numpy()\n",
    "            pred_classes[img]=prediction.xyxy[0][:,5].detach().numpy()\n",
    "    \n",
    "    return pred_scores,pred_boxes,pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions for different test sets\n",
    "#Take combinations of 2\n",
    "pred_confs=[]\n",
    "pred_boxes=[]\n",
    "pred_classes=[]\n",
    "\n",
    "test_paths=[PATH_TO_TEST,PATH_TO_INPAINT]\n",
    "test_imgids=[test_ids,inpaint_ids]\n",
    "\n",
    "for i in range(2):\n",
    "    confs_scores,box_preds,cls_pred=get_predictions(test_imgids[i],test_paths[i])\n",
    "\n",
    "    pred_confs.append(confs_scores)\n",
    "    pred_boxes.append(box_preds)\n",
    "    pred_classes.append(cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for weighted box fusion\n",
    "def perform_wbf(pred_confs_models,pred_boxes_models,pred_classes_models):\n",
    "    wbf_boxes_dict=dict()\n",
    "    wbf_scores_dict=dict()\n",
    "\n",
    "    for image_id in test_ids:\n",
    "        if '_' not in image_id:\n",
    "            res_array=np.array([1024,1024,1024,1024]) #1024X1024 images\n",
    "\n",
    "            all_model_boxes=[]\n",
    "            all_model_scores=[]\n",
    "            all_model_classes=[]\n",
    "\n",
    "            for boxes,scores,classes in zip(pred_boxes_models,pred_confs_models,pred_classes_models):\n",
    "                pred_boxes_norm=(boxes[image_id]/res_array).clip(min=0.,max=1.)\n",
    "                scores_model=scores[image_id]\n",
    "                classes_model=classes[image_id]\n",
    "\n",
    "                all_model_boxes.append(pred_boxes_norm)\n",
    "                all_model_scores.append(scores_model)\n",
    "                all_model_classes.append(classes_model)\n",
    "            \n",
    "            boxes,scores,labels=weighted_boxes_fusion(all_model_boxes,all_model_scores,all_model_classes,weights=None,iou_thr=0.5,skip_box_thr=0.30)\n",
    "\n",
    "            final_score_ids=np.where(scores>0.28)[0]\n",
    "            final_boxes=boxes[final_score_ids]\n",
    "            final_scores=scores[final_score_ids]\n",
    "\n",
    "            final_boxes=(final_boxes*res_array).clip(min=[0,0,0,0],max=[1023,1023,1023,1023])\n",
    "\n",
    "            final_boxes=final_boxes.astype(\"int\")\n",
    "\n",
    "            final_boxes[:,2:]=final_boxes[:,2:]-final_boxes[:,:2]\n",
    "            wbf_boxes_dict[image_id]=final_boxes.tolist()\n",
    "            wbf_scores_dict[image_id]=np.expand_dims(np.round(final_scores,5),axis=-1).tolist()\n",
    "    \n",
    "    return wbf_boxes_dict,wbf_scores_dict\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_dict_wbf,scores_dict_wbf=perform_wbf(pred_confs,pred_boxes,pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw the outputs using opencv tools\n",
    "def draw_bbox_conf(image,boxes,scores,color=(255,0,0),thickness=-1):\n",
    "    overlay=image.copy()\n",
    "\n",
    "    font_size=0.25+0.07*min(overlay.shape[:2])/100\n",
    "    font_size=max(font_size,0.5)\n",
    "    font_size=min(font_size,0.8)\n",
    "    text_offset=7\n",
    "\n",
    "    for box,score in zip(boxes,scores):\n",
    "        xmin=box[0]\n",
    "        ymin=box[1]\n",
    "        xmax=box[0]+box[2]\n",
    "        ymax=box[1]+box[3]\n",
    "\n",
    "        overlay=cv2.reactangle(overlay,(xmin,ymin),(xmax,ymax),color,thickness)\n",
    "        display_text=f\"wheat_head: {score[0]:.2f}\"\n",
    "        (text_width,text_height),_=cv2.getTextSize(display_text,cv2.FONT_HERSHEY_SIMPLEX,font_size,2)\n",
    "\n",
    "        cv2.rectangle(overlay,(xmin,ymin),(xmin+text_width+text_offset,ymin-text_height-int(15*font_size)),color,thickness=-1)\n",
    "\n",
    "        overlay=cv2.putText(overlay,display_text,(xmin+text_offset,ymin-int(10*font_size)),cv2.FONT_HERSHEY_SIMPLEX,font_size,(255,255,255),2,lineType=cv2.LINE_AA)\n",
    "\n",
    "    return cv2.addWeighted(overlay,0.75,image,0.25,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
